{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Copy of mapreduce1-word_count.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Migaalee/hadoop/blob/main/Copy_of_mapreduce1_word_count.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1lYTgUEfubc"
      },
      "source": [
        "# Python MapReduce Example\n",
        "\n",
        "Word count implemented in pure Python.\n",
        "\n",
        "This notebook exemplifies the execution of a map-reduce program in Python, using Hadoop.\n",
        "In this example, hadoop runs in standalone mode and reads data from the local filesystem, while in cluster mode data is read typically from HDFS dsitributed file system.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uczIQQVnfubd"
      },
      "source": [
        "### Download the dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "W0ecDPlsfubd",
        "outputId": "b0046a64-ca22-45ba-d3fe-879093960e5f"
      },
      "source": [
        "!wget -O os_maias.txt https://www.dropbox.com/s/n24v0z7y79np319/os_maias.txt?dl=0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-12 14:13:43--  https://www.dropbox.com/s/n24v0z7y79np319/os_maias.txt?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.68.1, 2620:100:6024:1::a27d:4401\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.68.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/n24v0z7y79np319/os_maias.txt [following]\n",
            "--2020-10-12 14:13:43--  https://www.dropbox.com/s/raw/n24v0z7y79np319/os_maias.txt\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc7f7f2e20bee4ec16561cdf66b9.dl.dropboxusercontent.com/cd/0/inline/BBJubJfDyLPKJvmbdSLG1o4_oNSEIBD23YY9ppDsBnviiEqSxrOXgEFxeWr2ohKnvaJHmCYpFCQV8LVKdtfIJyurRGp4zaPs2cibR8EQwZrQog/file# [following]\n",
            "--2020-10-12 14:13:44--  https://uc7f7f2e20bee4ec16561cdf66b9.dl.dropboxusercontent.com/cd/0/inline/BBJubJfDyLPKJvmbdSLG1o4_oNSEIBD23YY9ppDsBnviiEqSxrOXgEFxeWr2ohKnvaJHmCYpFCQV8LVKdtfIJyurRGp4zaPs2cibR8EQwZrQog/file\n",
            "Resolving uc7f7f2e20bee4ec16561cdf66b9.dl.dropboxusercontent.com (uc7f7f2e20bee4ec16561cdf66b9.dl.dropboxusercontent.com)... 162.125.68.15, 2620:100:6024:15::a27d:440f\n",
            "Connecting to uc7f7f2e20bee4ec16561cdf66b9.dl.dropboxusercontent.com (uc7f7f2e20bee4ec16561cdf66b9.dl.dropboxusercontent.com)|162.125.68.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1292368 (1.2M) [text/plain]\n",
            "Saving to: ‘os_maias.txt’\n",
            "\n",
            "os_maias.txt        100%[===================>]   1.23M  6.35MB/s    in 0.2s    \n",
            "\n",
            "2020-10-12 14:13:45 (6.35 MB/s) - ‘os_maias.txt’ saved [1292368/1292368]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Jm2rXvDfubh"
      },
      "source": [
        "## WordCount Example\n",
        "Read the words from input and count them.\n",
        "\n",
        "The processing is split into two steps:\n",
        "\n",
        "+ The mapper emits for each line the number of words\n",
        "+ The reduces sums all the tuples produced by the mapper stage..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JNKe6mAfubi"
      },
      "source": [
        "### Mapper\n",
        "\n",
        "By starting an element with \"%%file\", you are specifying that when run, the contents are written to the local disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZK6q1jWfubi",
        "outputId": "bd2d3712-dd7a-46ff-f6fd-8f98a5af4801"
      },
      "source": [
        "%%file mapper.py\n",
        "#!/usr/bin/env python\n",
        "\n",
        "# import sys\n",
        "import sys\n",
        "# import string library function  \n",
        "import string  \n",
        "\n",
        "# input comes from STDIN (standard input)\n",
        "for line in sys.stdin:\n",
        "    # remove leading and trailing whitespace\n",
        "    line = line.strip()\n",
        "    # remove punctuation characters\n",
        "    line = line.translate(str.maketrans('', '', string.punctuation+'«»'))\n",
        "    # split the line into words\n",
        "    words = line.split()\n",
        "    print('words\\t%s' % len(words))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing mapper.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQVKQjUqfubm"
      },
      "source": [
        "### Reducer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xzPkx7Nfubm",
        "outputId": "5f1b9a08-fb3e-4625-b651-c18f6110e7c7"
      },
      "source": [
        "%%file reducer.py\n",
        "#!/usr/bin/env python\n",
        "\n",
        "import sys\n",
        "\n",
        "total_count = 0\n",
        "\n",
        "# input comes from STDIN\n",
        "for line in sys.stdin:\n",
        "    # remove leading and trailing whitespace\n",
        "    line = line.strip()\n",
        "\n",
        "    # parse the input we got from mapper.py\n",
        "    key, count = line.split('\\t', 1)\n",
        "\n",
        "    # convert count (currently a string) to int\n",
        "    count = int(count)\n",
        "\n",
        "    total_count += count\n",
        "\n",
        "print('words\\t%s' % (total_count))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing reducer.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5pY1e4zfubp"
      },
      "source": [
        "### Local execution\n",
        "\n",
        "The scripts can be tested using just the unix shell, as follows..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUP92rUUfubq"
      },
      "source": [
        "#### Make the scripts executable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtsAQs15fubr"
      },
      "source": [
        "!chmod a+x mapper.py && chmod a+x reducer.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgfP6YGbfubu"
      },
      "source": [
        "#### Execute\n",
        "\n",
        "The execution workflow is as follows:\n",
        "\n",
        "+ The input file is piped into the input of the mapper;\n",
        "+ The output the mapper is sorted;\n",
        "+ The sorted output of the mapper is fed to the reducer stage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YYn-lJCfubv",
        "outputId": "b880a287-3d2c-446f-8565-1d16846eeb38"
      },
      "source": [
        "!cat \"os_maias.txt\" | ./mapper.py | sort -k1,1 | ./reducer.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "words\t213359\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icKyMsxIfuby"
      },
      "source": [
        "### Hadoop standalone mode execution\n",
        "\n",
        "For executing in an hadoop cluster, input data should be moved into an HDFS directory. For executing in standalone mode, data can be read from the local filesystem. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G90P8YD8fubz"
      },
      "source": [
        "\n",
        "The output directory needs to be cleared..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_VZOGQPfub0"
      },
      "source": [
        "rm -rf results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wogf9Arafub3"
      },
      "source": [
        "#### Submitting the job\n",
        "\n",
        "The _hadoop_ command is used to submit the mapreduce job to the cluster..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIwmxKSofub4",
        "outputId": "3de798c5-f89b-43ee-ad21-4adee28c2e35"
      },
      "source": [
        "!hadoop jar /opt/hadoop-3.2.0/share/hadoop/tools/lib/hadoop-*streaming*.jar -files mapper.py,reducer.py -mapper mapper.py -reducer reducer.py -input os_maias.txt -output results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-12 14:14:27,322 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2020-10-12 14:14:27,516 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2020-10-12 14:14:27,516 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2020-10-12 14:14:27,555 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2020-10-12 14:14:27,816 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2020-10-12 14:14:27,836 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2020-10-12 14:14:28,127 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local188657134_0001\n",
            "2020-10-12 14:14:28,128 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2020-10-12 14:14:28,475 INFO mapred.LocalDistributedCacheManager: Localized file:/home/jovyan/work/mapper.py as file:/tmp/hadoop-jovyan/mapred/local/job_local188657134_0001_8035d043-6753-422e-b584-2672be9c4446/mapper.py\n",
            "2020-10-12 14:14:28,499 INFO mapred.LocalDistributedCacheManager: Localized file:/home/jovyan/work/reducer.py as file:/tmp/hadoop-jovyan/mapred/local/job_local188657134_0001_53d22fc1-037f-4d5c-baf8-b157de481eeb/reducer.py\n",
            "2020-10-12 14:14:28,653 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2020-10-12 14:14:28,655 INFO mapreduce.Job: Running job: job_local188657134_0001\n",
            "2020-10-12 14:14:28,683 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2020-10-12 14:14:28,686 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2020-10-12 14:14:28,691 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2020-10-12 14:14:28,691 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2020-10-12 14:14:28,771 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2020-10-12 14:14:28,777 INFO mapred.LocalJobRunner: Starting task: attempt_local188657134_0001_m_000000_0\n",
            "2020-10-12 14:14:28,809 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2020-10-12 14:14:28,810 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2020-10-12 14:14:28,845 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2020-10-12 14:14:28,862 INFO mapred.MapTask: Processing split: file:/home/jovyan/work/os_maias.txt:0+1292368\n",
            "2020-10-12 14:14:28,884 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2020-10-12 14:14:28,966 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2020-10-12 14:14:28,967 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2020-10-12 14:14:28,967 INFO mapred.MapTask: soft limit at 83886080\n",
            "2020-10-12 14:14:28,967 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2020-10-12 14:14:28,967 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2020-10-12 14:14:28,970 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2020-10-12 14:14:28,975 INFO streaming.PipeMapRed: PipeMapRed exec [/home/jovyan/work/./mapper.py]\n",
            "2020-10-12 14:14:28,985 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2020-10-12 14:14:28,986 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2020-10-12 14:14:28,986 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2020-10-12 14:14:28,987 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2020-10-12 14:14:28,988 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2020-10-12 14:14:28,988 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2020-10-12 14:14:28,988 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2020-10-12 14:14:28,989 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2020-10-12 14:14:28,989 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2020-10-12 14:14:28,990 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2020-10-12 14:14:28,992 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2020-10-12 14:14:28,993 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2020-10-12 14:14:29,034 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2020-10-12 14:14:29,035 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2020-10-12 14:14:29,048 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2020-10-12 14:14:29,113 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2020-10-12 14:14:29,174 INFO streaming.PipeMapRed: Records R/W=1749/1\n",
            "2020-10-12 14:14:29,402 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2020-10-12 14:14:29,402 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2020-10-12 14:14:29,406 INFO mapred.LocalJobRunner: \n",
            "2020-10-12 14:14:29,406 INFO mapred.MapTask: Starting flush of map output\n",
            "2020-10-12 14:14:29,406 INFO mapred.MapTask: Spilling map output\n",
            "2020-10-12 14:14:29,406 INFO mapred.MapTask: bufstart = 0; bufend = 51885; bufvoid = 104857600\n",
            "2020-10-12 14:14:29,406 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26190892(104763568); length = 23505/6553600\n",
            "2020-10-12 14:14:29,476 INFO mapred.MapTask: Finished spill 0\n",
            "2020-10-12 14:14:29,501 INFO mapred.Task: Task:attempt_local188657134_0001_m_000000_0 is done. And is in the process of committing\n",
            "2020-10-12 14:14:29,509 INFO mapred.LocalJobRunner: Records R/W=1749/1\n",
            "2020-10-12 14:14:29,510 INFO mapred.Task: Task 'attempt_local188657134_0001_m_000000_0' done.\n",
            "2020-10-12 14:14:29,526 INFO mapred.Task: Final Counters for attempt_local188657134_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1469920\n",
            "\t\tFILE: Number of bytes written=759567\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=5877\n",
            "\t\tMap output records=5877\n",
            "\t\tMap output bytes=51885\n",
            "\t\tMap output materialized bytes=63645\n",
            "\t\tInput split bytes=87\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=5877\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=10\n",
            "\t\tTotal committed heap usage (bytes)=189267968\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=1292368\n",
            "2020-10-12 14:14:29,526 INFO mapred.LocalJobRunner: Finishing task: attempt_local188657134_0001_m_000000_0\n",
            "2020-10-12 14:14:29,526 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2020-10-12 14:14:29,541 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2020-10-12 14:14:29,542 INFO mapred.LocalJobRunner: Starting task: attempt_local188657134_0001_r_000000_0\n",
            "2020-10-12 14:14:29,556 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2020-10-12 14:14:29,558 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2020-10-12 14:14:29,558 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2020-10-12 14:14:29,566 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@4ac15eb4\n",
            "2020-10-12 14:14:29,568 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2020-10-12 14:14:29,596 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=325163424, maxSingleShuffleLimit=81290856, mergeThreshold=214607872, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2020-10-12 14:14:29,599 INFO reduce.EventFetcher: attempt_local188657134_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2020-10-12 14:14:29,637 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local188657134_0001_m_000000_0 decomp: 63641 len: 63645 to MEMORY\n",
            "2020-10-12 14:14:29,647 INFO reduce.InMemoryMapOutput: Read 63641 bytes from map-output for attempt_local188657134_0001_m_000000_0\n",
            "2020-10-12 14:14:29,649 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 63641, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->63641\n",
            "2020-10-12 14:14:29,655 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2020-10-12 14:14:29,657 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2020-10-12 14:14:29,657 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2020-10-12 14:14:29,670 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2020-10-12 14:14:29,671 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 63633 bytes\n",
            "2020-10-12 14:14:29,685 INFO mapreduce.Job: Job job_local188657134_0001 running in uber mode : false\n",
            "2020-10-12 14:14:29,693 INFO reduce.MergeManagerImpl: Merged 1 segments, 63641 bytes to disk to satisfy reduce memory limit\n",
            "2020-10-12 14:14:29,696 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2020-10-12 14:14:29,698 INFO reduce.MergeManagerImpl: Merging 1 files, 63645 bytes from disk\n",
            "2020-10-12 14:14:29,701 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2020-10-12 14:14:29,701 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2020-10-12 14:14:29,703 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 63633 bytes\n",
            "2020-10-12 14:14:29,704 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2020-10-12 14:14:29,716 INFO streaming.PipeMapRed: PipeMapRed exec [/home/jovyan/work/./reducer.py]\n",
            "2020-10-12 14:14:29,722 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2020-10-12 14:14:29,723 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2020-10-12 14:14:29,760 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2020-10-12 14:14:29,762 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2020-10-12 14:14:29,766 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2020-10-12 14:14:29,793 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2020-10-12 14:14:29,835 INFO streaming.PipeMapRed: Records R/W=5877/1\n",
            "2020-10-12 14:14:29,843 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2020-10-12 14:14:29,844 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2020-10-12 14:14:29,846 INFO mapred.Task: Task:attempt_local188657134_0001_r_000000_0 is done. And is in the process of committing\n",
            "2020-10-12 14:14:29,847 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2020-10-12 14:14:29,848 INFO mapred.Task: Task attempt_local188657134_0001_r_000000_0 is allowed to commit now\n",
            "2020-10-12 14:14:29,850 INFO output.FileOutputCommitter: Saved output of task 'attempt_local188657134_0001_r_000000_0' to file:/home/jovyan/work/results\n",
            "2020-10-12 14:14:29,852 INFO mapred.LocalJobRunner: Records R/W=5877/1 > reduce\n",
            "2020-10-12 14:14:29,852 INFO mapred.Task: Task 'attempt_local188657134_0001_r_000000_0' done.\n",
            "2020-10-12 14:14:29,853 INFO mapred.Task: Final Counters for attempt_local188657134_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1597242\n",
            "\t\tFILE: Number of bytes written=823237\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=63645\n",
            "\t\tReduce input records=5877\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=5877\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=189267968\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=25\n",
            "2020-10-12 14:14:29,857 INFO mapred.LocalJobRunner: Finishing task: attempt_local188657134_0001_r_000000_0\n",
            "2020-10-12 14:14:29,857 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2020-10-12 14:14:30,698 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2020-10-12 14:14:30,699 INFO mapreduce.Job: Job job_local188657134_0001 completed successfully\n",
            "2020-10-12 14:14:30,722 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=3067162\n",
            "\t\tFILE: Number of bytes written=1582804\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=5877\n",
            "\t\tMap output records=5877\n",
            "\t\tMap output bytes=51885\n",
            "\t\tMap output materialized bytes=63645\n",
            "\t\tInput split bytes=87\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=63645\n",
            "\t\tReduce input records=5877\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=11754\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=10\n",
            "\t\tTotal committed heap usage (bytes)=378535936\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=1292368\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=25\n",
            "2020-10-12 14:14:30,722 INFO streaming.StreamJob: Output directory: results\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VALA80bGfub7"
      },
      "source": [
        "#### Checking the results\n",
        "The result is stored in directory results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unwdtPUpfub7",
        "outputId": "2af9021d-7419-4e78-edb4-60eb890fcfb4"
      },
      "source": [
        "!cat results/part-*"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "words\t213359\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcVWHul-fub_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}